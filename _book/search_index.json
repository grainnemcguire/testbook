[["index.html", "MLRWP content prototype Introduction 0.1 Rendering book Misc points", " MLRWP content prototype MLR working party 20 June 2021 Introduction This is experimental work on ways of displaying MLRWP content. For the actual content itself, refer to https://institute-and-faculty-of-actuaries.github.io/mlr-blog/ We are considering collating MLRWP posts into an online book so they are more easily located This is a prototype of how it might look 0.1 Rendering book Can either use the preview book add-in or bookdown::render_book('index.Rmd', 'bookdown::gitbook') to render on demand. Misc points will need to use renv to ensure that this can be run - each time it is updated, it will need to be rebuilt. "],["mlrwp.html", "1 Machine Learning in Reserving Working Party 1.1 DISCLAIMER", " 1 Machine Learning in Reserving Working Party The General Insurance Machine Learning in Reserving working party is an international group of over 40 actuaries, bringing together experts in this field from around the globe. Our starting premise is that whilst machine learning techniques are widespread in pricing, they are not being adopted on the ground in reserving (certainly in the UK). The idea of the working party is to help move this forward, by identifying what the barriers are, communicating any benefits, and helping develop the research techniques in pragmatic ways. At the same time we understand the resource and time pressures that reserving actuaries are under and the aim is not to replace existing reserving methods per se, but to start the journey to understanding if and how machine learning may help us in our day to day work. Our intention is to develop and undertake our own research. To this end, we have a number of workstreams addressing different issues. Currently these are: Foundations Literature Review Survey Data Research We anticipate adding additional workstreams covering issues such as pragmatic considerations, and trust and ethics, as our research develops. Chair: Sarah MacDonnell Membership: 48 Established: 2019 1.1 DISCLAIMER The views expressed on this site are those of invited contributors and not necessarily those of the Institute and Faculty of Actuaries. The Institute and Faculty of Actuaries do not endorse any of the views stated, nor any claims or representations made in this publication and accept no responsibility or liability to any person for loss or damage suffered as a consequence of their placing reliance upon any view, claim or representation made in this publication. The information and expressions of opinion contained in this publication are not intended to be a comprehensive study, nor to provide actuarial advice or advice of any nature and should not be treated as a substitute for specific advice concerning individual situations. On no account may any part of this publication be reproduced without the written permission of the Institute and Faculty of Actuaries. "],["purpose.html", "2 Purpose of the book 2.1 Introduction 2.2 Blog", " 2 Purpose of the book This book is a prototype! This book gathers together our various blog posts into one place for more convenient future access. For up to date posts, and all historical ones, see our website 2.1 Introduction by Sarah MacDonnell Its probably fair to say that we are living in the era of big data and machine learning. In the actuarial world machine learning (ML) has certainly made inroads into personal lines pricing - tight margins and high competitiveness create a large incentive to extract as much value and insight from data as possible. It appears that there has been less use of machine learning in reserving, possibly due to less obvious or immediate competitive advantages. However we think there are significant potential benefits to be had from introducing machine learning into reserving. Such techniques applied to richer, more detailed and broader datasets may allow us to gain insights we have not been able to conceive of before; be it as new segmentations, operational improvements, early warning systems, cost efficiencies, or revelations into claim life cycles, settlement patterns or consumer behaviour. There may also be operational efficiencies to be gained; freeing time for targeted deep dives, allowing more frequent updates, or benefitting those with very large numbers of diverse classes. The General Insurance Machine Learning in Reserving Working Party (MLR WP) was established in 2019 with the aim of assisting the adoption of ML techniques in reserving. We are lucky to have a large and active membership comprising data scientists and academics as well as reserving and pricing actuaries. Some members of the working party have already written papers in this field, and we have strong representation in countries across the world (including the UK, Europe, the US, Australia, China, India and Africa). Our ambition is to become a global hub for this field; providing resources and bringing researchers together. To help us achieve this we currently have the following workstreams: Foundations: to provide useful educational resources, including sharing of code Literature Review: to review and promote relevant papers (and help us bring together the best ideas that are out there) Survey: to understand what is currently being done on the ground, and identify any barriers Data: to collate and promote sources of data that are available to help further research Research: to undertake our own research projects. Further details of each of these workstreams is available here. Additional workstreams will come on line (for example covering more practical considerations such as interpretation and communication of models, and ethics) as the working party progresses. 2.2 Blog Recognising that ML is a fast-moving field, we have set up a website and blog to share our work. Our intention is to post articles regularly from each of the workstreams. Initially it is anticipated that most of the articles will be from the Foundations workstream and aimed at those starting their journey in ML, but in time all current and future workstreams intend to produce material. This book gathers together our posts. See the blog for more recent articles. "],["f-intro.html", "3 Introduction", " 3 Introduction The foundations section covers posts that fall under the foundations workstream. "],["getting-started-f-getting-started.html", "4 Getting started {f-getting-started} 4.1 Programming language choice - R or Python? 4.2 Getting started with ML 4.3 Foundations workstream", " 4 Getting started {f-getting-started} This article was originally posted here and authored by Nigel Carpenter and Grainne McGuire Getting started with data science and machine learning (ML) has never been easier or harder. Easier in the sense that there are a wealth of resources available online, but harder in that it can be difficult to know where to start. The Foundations workstream of the MLR working party aims to provide some signposts along the machine learning journey, with a focus on material and examples that are relevant to reserving. 4.1 Programming language choice - R or Python? First bit of advice is dont get bogged down in the language wars! Both have advantages and disadvantages. If youre getting started with ML, then you want to use the language that you will find easiest to quickly learn so that you can focus on the techniques rather than your code syntax and deciphering cryptic error messages. If your workplace uses one language rather than the other then it will likely make sense to select that language. However, if you have no prior experience of either language and your reason for learning one of them is to gain a practical understanding of data science then the following guidance may be helpful. If your academic background is more statistics than computer science then R may better suit your background than Python. If your aim is to learn machine learning then either language is well suited and both have lots of free learning resources If your aim is to learn leading edge deep neural network techniques, Python is the communitys preferred language and has many more learning resources than R Management of dependencies (i.e. the particular versions of the language and packages used) in Python can be quite important. Particularly in cutting edge applications like neural networks, it may be necessary to use particular versions of packages to ensure that code runs correctly. But even simpler applications, like graphing can fall prey to this, where, for example, plotting code will run for one version of matplotlib but not for a more recent version. For this reason, newcomers to Python may need to get to grips with package management via environments sooner rather than later. Although the same issues can arise in R, anecdotally they appear less often, perhaps because the R core team emphasises backwards compatibility. Therefore, code examples in R are more likely to run without modification. Ultimately, if you use either language for practical applications, you should consider dependency No matter what language you select, if you start using applications in practice you should consider managing your dependencies appropriately to ensure that code from the past continues to work in future. 4.2 Getting started with ML 4.2.1 Data You may have heard the saying that machine learning is at least 80% data, 20% modelling and that is generally true. The data frequently are the distinguishing factor between good models and great models. As actuaries, we already have a sound basis in handling data - we are trained to be sceptical of data and to question unusual patterns. Many of us have experience of collecting and cleaning data from different sources for reserving and pricing jobs and we understand the importance of checks and reconciliations. Therefore, the main learning curves for actuaries in relation to data are likely to involve: sourcing external data, e.g. via web-scraping. This also includes learning to access SQL (or similar) data bases. cleaning and processing data in your language of choice (pandas in python may help here; data.table or tidyverse in R) 4.2.2 Methods For those new to ML, our advice is to approach learning techniques from a familiar direction. As actuaries most of us should have some familiarity with Generalised Linear Models (GLMs) if we have studied General Insurance Pricing, so this suggests a starting point. The steps are then: gain familiarity with using GLMs to apply traditional triangular reserving techniques. There are many papers and ready made R packages to help (e.g. glmReserve() in the R ChainLadder package). apply regularised GLMs (e.g. apply lasso or ridge regression or mixture of both) to fit something that looks like a GLM but in a machine learning way - in these methods, the machine selects features for inclusion in the model. Move onto tree based methods like decision trees and random forests and from there into more advanced ML techniques such as Gradient Boosting Machines (GBMs). Note that GBMs include XGBoost, which is very popular among data scientists. At this point you could then move to learning about neural networks. While these are likely to be very useful in the future, they are the least accessible both in terms of the actual methods - you need a good grasp of deep neural networks to understand the techniques most likely to be useful (such as recurrent neural networks - see, e.g., Deep Triangle) and also in terms of the data and hardware needed to get good results. You often need lots of data and high end computer equipment (or cloud based virtual machines) to train these models. 4.3 Foundations workstream Our task is to provide some stepping stones to getting started with ML. On our Workstream page, we maintain a list of useful resources, including a link to those compiled by a complimentary IFoA group, the Data Science Working Party. We also have a planned series of articles on getting started in Machine Learning which will be posted over the next few weeks. Topics will include getting started with R, data manipulation, graphing, and fitting various methods to claims reserving examples (following the steps outlined above). Note that initially our articles will mainly be in R, but we hope to extend content to including python examples in the future. So check back regularly and search for the foundations tag or look at the list of posts to view our content. "],["f-intro-r.html", "5 Introduction to R 5.1 Why use R? 5.2 How do I get started? 5.3 Hello World! 5.4 Conclusion", " 5 Introduction to R This article was written by Wan Hsien Heah and originally published on 21 Sep 2020 There are several different data science tools available today. R is one such tool that is used quite widely, free and easy to get started with. R started out as an experiment at the University of Auckland to see if an existing language called S could be improved such that code would be cleaner and easier to run. It has come a long way since its humble beginnings in the mid-90s. Today, R is used for a variety of tasks including complex capital model calculations and running machine learning algorithms in pricing and reserving. 5.1 Why use R? There are a number of benefits to using R. These are: Free: There is no license fee associated with using R (although beware that there may be license restrictions with using R packages). Widely used: R has a large user base, with data scientists, data miners, statisticians and actuaries using the tool around the world. That means that it only takes a quick search on the internet to get help with any problem that you might be experiencing with coding in R. An R-ready workforce: R is now a part of the UK Institute and Faculty of Actuaries (IFoA) exam syllabus. Increasingly, more and more university students are also learning R as part of the undergraduate course in statistics and actuarial science. R is fast: R is usually faster at computational problems compared to traditional tools like Excel. For example, R can generate a million random standard Normal samples in less than a second. This is because of the way R is structured and the way it works. R can even be faster than server-based languages like SQL. Packages: There are many freely available reusable R code libraries available online called packages. These packages have been built by other R users and perform a variety of calculations or processes which means that you do not have to start from scratch all the time. The crowd-sourced sharing of packages helps accelerate development time in R. We will cover some of the more useful packages for actuaries in our future blog posts. 5.2 How do I get started? R is available to download from the Comprehensive R Archive Network (CRAN) at https://cran.r-project.org/. CRAN is a network of servers around the world that contains copies of the R application and the various packages which is continuously being updated and maintained. To get started, you will first need to download R from the CRAN homepage. Select the appropriate download for your operating system and follow the usual instructions to compile. You will usually want to download the most recent version, but occasionally you may need to download older versions if, for example, a package you need to use has not yet been updated to run on the latest version of R. The instructions for each platform will be on the website as you click the version of R that you need. For this blog post, we will focus on the Windows version of R as it is the more common platform used by actuaries. For first time users, you will need to install base R. It is important to note that base R is a console application i.e. the main interaction is through text. Most users will want to use an Integrated Development Environment (IDE) or a user interface. So, after downloading R, you should consider installing an IDE such as RStudio. You should install R first, then RStudio. RStudio provides a graphical user interface (GUI) that provides a more visual interaction when using R. You can download a free desktop version from the RStudio website: https://rstudio.com/. There are also paid options for support and for running and maintaining a server. Once you have both R and RStudio installed, you are ready to begin coding. 5.3 Hello World! Now that you have the minimum software required installed, you can start coding. Getting started can be quite daunting as there are various schools of thought on how to use R and which packages. Base R is what comes built into R. There are also a growing series of data manipulation packages commonly known as tidyverse which consists of packages developed and maintained by RStudio. Beyond that there are other data manipulation packages such as data.table. Each have their advantages and disadvantages depending on the precise operation that you would like to perform There are several free resources available for learning R which you can find on the internet. We list a few of them here: R for Data Science R-bloggers - an aggregator of online articles about R Rweekly.org - a weekly curated list of updates from the R community For more advanced users, useful reference guides are: Advanced R, first edition more advanced techniques using mainly baseR Advanced R, second edition the updated book, which incorporates some of the tidyverse ideas. Additionally, you could also sign up to online courses from Coursera, Stanford, CodeAcademy, etc. that teach R with the added benefit of certification at the end. 5.4 Conclusion There are, of course, other free statistical tools available out there. We have chosen to focus on R as it is one of the more prevalent programming languages being used in the actuarial space. R brings with it many benefits which you may find useful in the work that you do. Have a go and try it out! "],["f-glms.html", "6 Reserving with GLMs 6.1 Introduction 6.2 Data 6.3 Chain ladder model 6.4 Refining the model 6.5 Reviewing this example 6.6 Practical use of GLMs in traditional reserving", " 6 Reserving with GLMs add author and publication date, fix up links An aim of the MLR working party is to promote the use of machine learning (ML) in reserving. So why then are we talking about using GLMs for reserving? Well, as noted in our introductory post, we consider that getting familiar with using GLMs for reserving is a good place to begin your ML journey - GLMs should already be familiar from pricing so making the switch to reserving with GLMs is a useful first step. This is longer than many of our recent articles. However, this is because it contains a full worked example of the modelling and reserving process for a data set - as per the objectives of the working party, we want to include practical coding examples in our articles. There are also some concluding comments about the wider applicability of this type of process. 6.1 Introduction The first time I did a reserving job (back in 2001) I used GLMs. Coming from a statistical background and being new to the actuarial workplace at the time, this didnt seem unusual to me. Since then, most of my reserving jobs have used GLMs - personally I find it a lot easier and less error-prone than working with excel templates. Also, once you know what you are doing, you can do everything with a GLM that you can do with an excel-based model, and then more. However, some people reading this article may be new to the idea of using GLMs in reserving. So Im going to use an example where we start with a chain ladder model, fitted as a GLM and then explore the additional features that we can add using a GLM. All the R code will be shared here. The material is mostly based on a 2016 CAS monograph Stochastic Loss Reserving Using Generalized Linear Models that I co-authored with Greg Taylor, and an accompanying personal blog post that works through replicating some of the modelling in the monograph. Take a look at these if you want to see more about this example. Before we begin, lets attach the R packages that we need, and turn off scientific notation. library(here) # needed to download the data for now library(data.table) # manipulate the data library(ggplot2) # plot the data library(viridis) # plot colours that are friendly to colour blindness library(patchwork) # easily combine plots library(magrittr) # gives access to %&gt;% pipe library(kableExtra) # formats tables nicely options(scipen = 99) # get rid of scientific notation 6.2 Data The data used here were sourced from the Meyers and Shi (2011) database, and are the workers compensation triangle of the New Jersey Manufacturers Group. They are displayed in Section 1.3 of the monograph. Weve made a CSV file of the data (in long format) available here for convenience. If you want to load it in, then use the fread statement that points to the website address which is included in the comments below. acc_year dev_year cumulative incremental 1 1 41821 41821 1 2 76550 34729 1 3 96697 20147 1 4 112662 15965 1 5 123947 11285 1 6 129871 5924 1 7 134646 4775 1 8 138388 3742 1 9 141823 3435 1 10 144781 2958 2 1 48167 48167 2 2 87662 39495 2 3 112106 24444 2 4 130284 18178 2 5 141124 10840 2 6 148503 7379 2 7 154186 5683 2 8 158944 4758 2 9 162903 3959 3 1 52058 52058 3 2 99517 47459 3 3 126876 27359 3 4 144792 17916 3 5 156240 11448 3 6 165086 8846 3 7 170955 5869 3 8 176346 5391 4 1 57251 57251 4 2 106761 49510 4 3 133797 27036 4 4 154668 20871 4 5 168972 14304 4 6 179524 10552 4 7 187266 7742 5 1 59213 59213 5 2 113342 54129 5 3 142908 29566 5 4 165392 22484 5 5 179506 14114 5 6 189506 10000 6 1 59475 59475 6 2 111551 52076 6 3 138387 26836 6 4 160719 22332 6 5 175475 14756 7 1 65607 65607 7 2 110255 44648 7 3 137317 27062 7 4 159972 22655 8 1 56748 56748 8 2 96063 39315 8 3 122811 26748 9 1 52212 52212 9 2 92242 40030 10 1 43962 43962 So we have four columns: acc_year: accident year, numbered from 1 to 10 dev_year: development year, also numbered from 1 to 10 cumulative: cumulative payments to date incremental: incremental payments for that accident year, development year combination. We can also plot the data The data look quite well behaved - each year seems to have a similar development pattern. 6.3 Chain ladder model 6.3.1 Fitting the model Our first model will be the familiar chain ladder (volume all) model. The monograph (and references therein) note that certain types of GLM gives exactly the same result as the chain ladder so Im going to use that to get the chain ladder result. The specific model Im using that replicates the chain ladder result is the Over-dispersed Poisson (ODP) cross classified (cc) model (Sections 3.3.2 and 3.3.3 of the monograph). To apply the model, we will use the glm function from the base R stats package. The cross-classified model requires separate levels for each of accident and development year so we first make a factor version of these variates. Im also going to add a calendar year term (cal_year) for later use in model diagnostics. I use data.table for data manipulation. For those not familiar with it, := is an assignment operator and the syntax dt[, a := b] creates a new variable called a in the dt data.table (which is also a data.frame), and sets it equal to b. The comma at the start is there because the first part of a data.table command subsets the data and is left blank if there is no subsetting required. ## acc_year dev_year cumulative incremental acc_year_factor dev_year_factor cal_year ## 1: 1 1 41821 41821 1 1 1 ## 2: 1 2 76550 34729 1 2 2 ## 3: 1 3 96697 20147 1 3 3 ## 4: 1 4 112662 15965 1 4 4 ## 5: 1 5 123947 11285 1 5 5 ## 6: 1 6 129871 5924 1 6 6 Now we fit the model and look at the results via summary. The family is the quasipoisson - this is how we fit an ODP model with glm(). The link is log The formula is simply incremental ~ 0 + acc_year_factor + dev_year_factor The 0 tells glm() to fit a model without an intercept - which is how we fit the model in the monograph ## ## Call: ## glm(formula = &quot;incremental ~ 0 + acc_year_factor + dev_year_factor&quot;, ## family = quasipoisson(link = &quot;log&quot;), data = msdata) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -21.493 -5.534 0.000 5.136 21.059 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## acc_year_factor1 10.65676 0.03164 336.794 &lt; 0.0000000000000002 *** ## acc_year_factor2 10.79533 0.02994 360.507 &lt; 0.0000000000000002 *** ## acc_year_factor3 10.89919 0.02887 377.465 &lt; 0.0000000000000002 *** ## acc_year_factor4 10.98904 0.02808 391.326 &lt; 0.0000000000000002 *** ## acc_year_factor5 11.03883 0.02783 396.654 &lt; 0.0000000000000002 *** ## acc_year_factor6 11.01590 0.02855 385.867 &lt; 0.0000000000000002 *** ## acc_year_factor7 11.00808 0.02945 373.734 &lt; 0.0000000000000002 *** ## acc_year_factor8 10.89050 0.03266 333.463 &lt; 0.0000000000000002 *** ## acc_year_factor9 10.83613 0.03669 295.348 &lt; 0.0000000000000002 *** ## acc_year_factor10 10.69108 0.05104 209.454 &lt; 0.0000000000000002 *** ## dev_year_factor2 -0.20466 0.02276 -8.993 0.00000000009767316 *** ## dev_year_factor3 -0.74741 0.02819 -26.512 &lt; 0.0000000000000002 *** ## dev_year_factor4 -1.01667 0.03284 -30.954 &lt; 0.0000000000000002 *** ## dev_year_factor5 -1.45160 0.04214 -34.446 &lt; 0.0000000000000002 *** ## dev_year_factor6 -1.83254 0.05471 -33.495 &lt; 0.0000000000000002 *** ## dev_year_factor7 -2.14026 0.07150 -29.933 &lt; 0.0000000000000002 *** ## dev_year_factor8 -2.34827 0.09312 -25.218 &lt; 0.0000000000000002 *** ## dev_year_factor9 -2.51317 0.12673 -19.831 &lt; 0.0000000000000002 *** ## dev_year_factor10 -2.66449 0.19930 -13.369 0.00000000000000158 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 114.5364) ## ## Null deviance: 27479374.2 on 55 degrees of freedom ## Residual deviance: 4128.1 on 36 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 3 Im now going to save a more data.table version of the coefficient table in the glm_fit1 object - this will be used later and having the coefficients available in a data.table makes things easier. Ill call this coeff_table. ## parameter coeff_glm_fit1 ## 1: acc_year_factor1 10.65676 ## 2: acc_year_factor2 10.79533 ## 3: acc_year_factor3 10.89919 ## 4: acc_year_factor4 10.98904 ## 5: acc_year_factor5 11.03883 ## 6: acc_year_factor6 11.01590 6.3.2 Loss reserve Now well have a look at the loss reserve. If youve done the chain ladder calculations, you should find this gives the same answer. acc_year OCL 2 3398 3 8155 4 14579 5 22645 6 31865 7 45753 8 60093 9 80983 10 105874 Total 373346 6.3.3 Model diagnostics 6.3.3.1 Calculations So far, weve fitted a model and have calculated the loss reserve. We can do all this with the chain ladder algorithm. Now we start looking at some of the extras that using a GLM gives us - for a start the statistical model structure means that we can calculate residuals with known properties, assuming the assumptions underlying the model are valid. If the model assumptions are satisfied by the data, then the residuals should not have any remaining structure. Plots of residuals can therefore be useful to detect violations of model assumptions. Note that we need to be careful to use the right types of residuals - in many cases we want to use standardised deviance residuals: Deviance residuals because the more recognisable Pearson residuals (based on actual - fitted) are difficult to interpret for non-normal models. Standardised because the raw residuals are on different scales depending on the scale of the underlying values. Some more details may be found in Chapter 6 of the monograph and also in Chapter 5 of A Practitioners Introduction to Stochastic Reserving. Here we look at the following: Residual Scatterplots by linear predictor by accident, development and calendar years if the model assumptions are satisfied then the residuals should look homogeneous (or in laypersons language, like a random cloud), centred around zero) Heat map of actual vs fitted laid out in triangular form In this we get the actual/fitted ratio in each (acc, dev) cell (subject to lower and upper bounds of [0.5, 2]) and then plot the colour-coded triangle of the actual/fitted values heat maps are helpful to check for model fit and may help to identify missing interactions. We have to prepare the data by adding the fitted values and residuals. Because this model has a lot of parameters, there are two observations where the fitted is exactly equal to the actual  (acc_year=1, dev_year=10) and (acc_year=10, dev_year=1). This is because these observations have a unique parameter. The deviance calculations below return NaN (not a number) for these points, but the residual should really be 0 so this adjustment is made manually. Also add actual/fitted ratios and the log of these (restricted to the range [log(0.5), log(2)]) - these will be used for a heatmap later. The restricted range is used to generate easier to read shadings in the heat-map, while the conversion to log means that the shading scales will be similar intensity for \\(x\\)% and \\(1/x\\) % Technical note on residuals with glm() The residuals in a glm object accessed with $residuals are residuals used in the model fitting algorithm. For diagnostic purposes, we require the standardised deviance residuals. These are the signed square roots of the contribution of the ith observation to the deviance, divided by hat matrix values. The stats::rstandard() function may be used with glm objects to extract the standardised deviance residuals. ## acc_year dev_year cumulative incremental acc_year_factor dev_year_factor cal_year residuals1 fitted1 linear_predictor1 AvsF1 AvsF_restricted1 ## 1: 1 10 144781 2958 1 10 10 NaN 2958 7.992269 1 -0.0000000000000019984014 ## 2: 10 1 43962 43962 10 1 10 NaN 43962 10.691081 1 0.0000000000000008881784 Look at first 10 rows ## acc_year dev_year cumulative incremental acc_year_factor dev_year_factor cal_year residuals1 fitted1 linear_predictor1 AvsF1 ## 1: 1 1 41821 41821 1 1 1 -0.37704981 42478.725 10.656759 0.9845164 ## 2: 1 2 76550 34729 1 2 2 0.06821815 34616.808 10.452095 1.0032410 ## 3: 1 3 96697 20147 1 3 3 0.02211088 20117.514 9.909346 1.0014657 ## 4: 1 4 112662 15965 1 4 4 0.50192703 15368.757 9.640092 1.0387958 ## 5: 1 5 123947 11285 1 5 5 1.36344235 9948.355 9.205163 1.1343584 ## 6: 1 6 129871 5924 1 6 6 -1.13119533 6796.876 8.824218 0.8715769 ## 7: 1 7 134646 4775 1 7 7 -0.33754581 4996.553 8.516503 0.9556589 ## 8: 1 8 138388 3742 1 8 8 -0.56680264 4058.159 8.308485 0.9220929 ## 9: 1 9 141823 3435 1 9 9 -0.01379476 3441.253 8.143591 0.9981829 ## 10: 1 10 144781 2958 1 10 10 0.00000000 2958.000 7.992269 1.0000000 ## AvsF_restricted1 ## 1: -0.015604749951508145936 ## 2: 0.003235741327323749146 ## 3: 0.001464610789021573859 ## 4: 0.038062125103388820546 ## 5: 0.126067171138098593763 ## 6: -0.137451186377785167236 ## 7: -0.045354245283910396558 ## 8: -0.081109303248732236846 ## 9: -0.001818723883641001036 ## 10: -0.000000000000001998401 6.3.3.2 Plotting Now lets look at the residual scatterplots. In the linear predictor scatterplot, the points are colour coded so that the lighter points belong to the earlier development years, and the darker points belong to the later ones. These results are quite good - bear in mind there are only a small number of points so plots must be interpreted in relation to this. In particular: The residuals do not appear to fan out or fan in (once you take into account that later development years have small number of points) They appear centred around 0 Now construct and draw the heat map. Note that the colours are: blue (A/F = 50%) white (A/F = 100%) red (A/F = 200%) with shading for in-between values In a heat map for a reserving triangle, we look for a random scattering of red and blue points. This plot looks quite good (though well revisit this shortly). 6.4 Refining the model We could stop here - and just use the results from this model, which match those produced by the chain ladder. The diagnostics suggest that the model fits quite well. However, because this is a GLM, we have more options than just replicating the chain ladder. In particular, can we: identify simplifications to the model to make it more parsimonious (i.e. reduce the number of parameters)? identify any areas of poorer fit that may suggest missing model terms including interactions? 6.4.1 Simplifying the model First we consider if we can use a parametric shape for the accident and development year parameters. The end result should be something similar to the chain ladder approach but with far fewer parameters. 6.4.1.1 Accident year First plot the accident year parameters. Note that their shape closely resembles that of a parabola. This suggests that we can replace the 10 accident year parameters by the overall intercept an acc_year term an acc_year squared term So refit the model on this basis. Drop the 0 from the glm_fit1 formula to allow the model to have an intercept Replace the acc_year_factor term with the parabola terms. ## ## Call: ## glm(formula = &quot;incremental ~ acc_year + acc_year_2 + dev_year_factor&quot;, ## family = quasipoisson(link = &quot;log&quot;), data = msdata) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -28.5517 -5.1747 0.2691 4.5827 24.5421 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.470978 0.034414 304.264 &lt; 0.0000000000000002 *** ## acc_year 0.200075 0.014219 14.071 &lt; 0.0000000000000002 *** ## acc_year_2 -0.017907 0.001356 -13.210 &lt; 0.0000000000000002 *** ## dev_year_factor2 -0.205555 0.021276 -9.661 0.00000000000243 *** ## dev_year_factor3 -0.750108 0.026492 -28.314 &lt; 0.0000000000000002 *** ## dev_year_factor4 -1.014806 0.030982 -32.755 &lt; 0.0000000000000002 *** ## dev_year_factor5 -1.451958 0.039797 -36.484 &lt; 0.0000000000000002 *** ## dev_year_factor6 -1.830488 0.051662 -35.432 &lt; 0.0000000000000002 *** ## dev_year_factor7 -2.142154 0.067504 -31.734 &lt; 0.0000000000000002 *** ## dev_year_factor8 -2.352674 0.087924 -26.758 &lt; 0.0000000000000002 *** ## dev_year_factor9 -2.513722 0.119637 -21.011 &lt; 0.0000000000000002 *** ## dev_year_factor10 -2.660878 0.187820 -14.167 &lt; 0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 102.5776) ## ## Null deviance: 750824 on 54 degrees of freedom ## Residual deviance: 4427 on 43 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 3 We see in the coefficient table part of the summary that the two acc_year terms are highly significant. Now extract the coefficients and compare the previous and current fits. Remember that the intercept must be included in these calculations. Again, save the coefficient table as a data.table in the glm_fit2 object for later use. parameter coeff_glm_fit2 (Intercept) 10.4709782 acc_year 0.2000750 acc_year_2 -0.0179069 dev_year_factor2 -0.2055551 dev_year_factor3 -0.7501082 dev_year_factor4 -1.0148062 dev_year_factor5 -1.4519575 dev_year_factor6 -1.8304877 dev_year_factor7 -2.1421539 dev_year_factor8 -2.3526736 dev_year_factor9 -2.5137216 dev_year_factor10 -2.6608777 Now compare the past and current parameter estimates for accident year. This looks very good - the fit is very similar, but we have 7 fewer parameters. 6.4.1.2 Development year Now we do the same thing for development year Note that the glm_fit2 model (and the glm_fit1 model too) do not have a parameter for dev_year = 1 as this is the base level. This means that the parameter is really 0, so we must remember to include this. Looking at this plot, it appears that a straight line would fit quite well This fit would be improved by allowing the straight line to bend (have a knot) at dev_year = 7 So lets try this below note we actually fit dev_year - 1 rather than dev_year this means that the parameter estimate at dev_year = 1 is 0, just as it is in the glm_fit2 model, so it makes the results comparable if we fit dev_year, then the parameter estimate at dev_year=1 would be non-zero, so the two fits would be shifted relative to each other and we would need to adjust for that. ## ## Call: ## glm(formula = &quot;incremental ~ acc_year + acc_year_2 + dev_year_m1 + dev_year_ge_7&quot;, ## family = quasipoisson(link = &quot;log&quot;), data = msdata) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -25.301 -9.262 -2.080 5.893 42.841 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.509475 0.052096 201.734 &lt; 0.0000000000000002 *** ## acc_year 0.204224 0.021608 9.451 0.00000000000104 *** ## acc_year_2 -0.018295 0.002058 -8.891 0.00000000000719 *** ## dev_year_m1 -0.364073 0.008845 -41.160 &lt; 0.0000000000000002 *** ## dev_year_ge_7 0.238860 0.088426 2.701 0.00941 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 242.0614) ## ## Null deviance: 750824 on 54 degrees of freedom ## Residual deviance: 11879 on 50 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 4 Assuming the fit is satisfactory, our original model with 19 parameters has now been simplified to 5 parameters - much more parsimonious and robust. Lets check the fit by dev_year to see. This looks good. However dev_year = 2 is a bit underfit in the latest model, so we can add something to improve this fit (a term at dev_year=2) So refit and replot. Looks good! Fitting dev_year=2 better has also improved the tail fitting (dev_year&gt;7). 6.4.2 Identifying missing structure The second part of the model refining process involves checking for missing structure. Lets have a better look at the heat map, as it stands after the model simplification process Lets add some annotations to highlight some structure We see: development year 1, a distinct area of blue in the earlier accident years (A &lt; F), followed by red (A &gt; F) development year 2, a distinct area of red in the earlier accident years (A &gt; F), followed by blue (A &lt; F) development year 3, a possible progression from red to blue with increasing accident year (F increasing relative to A) development year 4, nearly all red (A &gt; F) This suggests the payment pattern has altered and can be accommodated by (mostly) interaction terms within the GLM. Consider adding the following terms: (development year = 1) * (accident year is between 1 and 6) (development year = 2) * (accident year is between 1 and 6) (development year = 3) * (accident year linear trend) (development year = 4) So, lets refit the model with terms to capture these and have a look at the heat map again ## ## Call: ## glm(formula = &quot;incremental ~ acc_year + acc_year_2 + dev_year_m1 + dev_year_ge_7 + dev_year_eq_2 + dev_year_eq_4 +\\n\\tdev_year_eq_1:acc_year_1_6 + dev_year_eq_2:acc_year_1_6 + dev_year_eq_3:acc_year &quot;, ## family = quasipoisson(link = &quot;log&quot;), data = msdata) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -13.7416 -3.8986 -0.4483 5.3380 13.4570 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.490384 0.032004 327.787 &lt; 0.0000000000000002 *** ## acc_year 0.206624 0.010871 19.007 &lt; 0.0000000000000002 *** ## acc_year_2 -0.018333 0.001140 -16.078 &lt; 0.0000000000000002 *** ## dev_year_m1 -0.368487 0.006911 -53.317 &lt; 0.0000000000000002 *** ## dev_year_ge_7 0.271988 0.044265 6.145 0.00000019 *** ## dev_year_eq_2 0.037485 0.024691 1.518 0.13596 ## dev_year_eq_4 0.052800 0.023821 2.217 0.03175 * ## dev_year_eq_1:acc_year_1_6 -0.067134 0.028071 -2.392 0.02102 * ## dev_year_eq_2:acc_year_1_6 0.127316 0.031042 4.101 0.00017 *** ## acc_year:dev_year_eq_3 -0.011262 0.003866 -2.913 0.00556 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 53.93331) ## ## Null deviance: 750824.1 on 54 degrees of freedom ## Residual deviance: 2426.9 on 45 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 3 This model should match that displayed in Table 7-5 of the monograph - and indeed it does (some very minor differences in parameter values - the model in the monograph was fitted in SAS). Look at the updated heat map again with the annotations - has the model resolved the identified issues? This looks much better. We should also look at the residual plots again These residuals do look better than those from the chain ladder model. 6.4.3 Loss reserve Now that we have a model, lets produce the estimate of the outstanding claims by accident year and in total. Take the lower triangle data [futdata] created above Add on the new variates we created Score the model on this data Summarise the results Create the data and score using predict(). Store the predicted values in the incremental column. ## acc_year dev_year cal_year acc_year_factor dev_year_factor incremental acc_year_2 dev_year_m1 dev_year_ge_7 dev_year_eq_1 dev_year_eq_2 dev_year_eq_3 ## 1: 2 10 12 2 10 3618.769 4 9 2.5 0 0 0 ## 2: 3 9 12 3 9 4470.907 9 8 1.5 0 0 0 ## 3: 3 10 13 3 10 4059.635 9 9 2.5 0 0 0 ## 4: 4 8 12 4 8 5324.841 16 7 0.5 0 0 0 ## 5: 4 9 13 4 9 4835.016 16 8 1.5 0 0 0 ## 6: 4 10 14 4 10 4390.250 16 9 2.5 0 0 0 ## dev_year_eq_4 acc_year_1_6 ## 1: 0 1 ## 2: 0 1 ## 3: 0 1 ## 4: 0 1 ## 5: 0 1 ## 6: 0 1 Get reserves by accident year and in total acc_year incremental 2 3619 3 8531 4 14550 5 22173 6 32458 7 45695 8 62955 9 79301 10 101212 The total reserve is 370493. 6.5 Reviewing this example Looking back over this example, what we have done is started with a chain ladder model and then shown how we can use a GLM to fit a more parsimonious model (i.e. fewer parameters). It may then be possible to reconcile the shape of the parametric fit by accident year to underlying experience in the book - here we saw higher payments in the middle accident years. Is this due to higher claims experience or higher premium volumes? Does this give us an insight that allows us to better extrapolate into the future when setting reserves? We have also used model diagnostics to identify areas of misfit and then used GLM interactions to capture these changes. 6.6 Practical use of GLMs in traditional reserving 6.6.1 Modelling The ideas in this simple example extend to more complex traditional scenarios. By traditional I mean that the data you have available to you are classified by accident (or underwriting), development and calendar periods only. First decide what you are going to model. Here we had a single model of incremental payments. However you could fit a Payments Per Claim Finalised (PPCF) model which consists of 3 submodels - numbers of claims incurred by accident period, number of claims finalised by (accident, development period) and payments per claim finalised by (accident, development period). Each of these could then be fitted by a GLM. For whatever youre modelling, you then pick the two triangle directions that you think are most critical for that experience. You cant include all 3 at the start since they are correlated. So, for PPCF submodels: for number of claims incurred models, accident and development period effects are likely to be where you start. numbers of claims finalised will usually depend on development period (type of claim) and calendar period (to take account of changes in claim settlement processes) for claim size models, you will probably want development and calendar period effects. For these models you could use operational time instead of development period to avoid changes in the timing of claims finalisations impacting your model. Then fit the models by starting with the modelled effects as factors and use methods such as those outlined above to reduce the number of parameters by using parametric shapes. Look for missing structure and consider adding interactions or (carefully) adding limited functions of the third triangular direction. Take advantage of GLM tools to refine your model. Use what you know about the portfolio to inform your model - if you know that there was a period of rapid claims inflation, then include that in your model. 6.6.2 Setting reserves It is possible to overlay judgement onto a GLMs predictions. At the end of the day, the predictions are just based on a mathematical formula. So, taking claims inflation as an example, if youve been seeing 5% p.a. over the last 3 years, but you think this is going to moderate going forward, then you can adjust the projections by removing the 5% p.a. rate into the future and replacing it with, say, 2% p.a. Once you get familiar with using GLMs, you might find it easier to incorporate judgement - the GLM can capture more granularity about past experience which in turn may make it easier to work out how things might change in future and how to numerically include these changes. 6.6.3 Additional References The references below have further examples of fitting GLMs in this way, and show how to capture quite complex experience. Although both use individual data, the methodology can be used in a similar manner for aggregate data. Loss Reserving with GLMs: A Case Study Individual Claim modelling of CTP data Predictive modeling applications in actuarial science, Frees and Derig, 2004 - in particular see Chapter 18 in Volume 1 and Chapter 3 in Volume 2. Please feel free to add references to other useful material in the comments. "],["introduction-3.html", "7 Introduction", " 7 Introduction In this section we compile the posts from the data workstream. "],["d-simulationmachine.html", "8 Data Maker - simulating data with simulationmachine 8.1 The right time 8.2 Make along with me 8.3 Little by little", " 8 Data Maker - simulating data with simulationmachine Post originally by John McCarthy https://institute-and-faculty-of-actuaries.github.io/mlr-blog/post/simulationmachine/ 8.1 The right time If you build a polygon from rods and hinges, what is the only shape to hold firm when you push on the edges? It is a triangle. Our three sided friend is everywhere in construction - look out the next time you walk past a pylon or bridge. We can think of the triangle as the shapes shape; irreducible, and good for approximating other shapes e.g. computer graphics represent complex surfaces by covering them in a mesh of small triangles and zooming out. In the world of insurance, if you zoom out far enough, individual claims data morphs into the familiar development triangle. The development triangle has the effect of watering claims data down into a thin porridge: any ten-year annual paid history - whether your portfolio contains dozens of claims or millions - is diluted to just 55 numbers for a reserve study. All fine for chain ladder methods, with a small appetite for bumps and edges, but machine learning algorithms are data hungry. If we want to test the full range of machine learning methods available then we need to start to zoom in on the triangle. Many actuaries can now source transactional data from tools managed by their employers, but this poses two problems: Company data cannot be released into the public domain for others to use, so the company actuary is unable to share the details of her research with outsiders. It is not unheard of for company data to contain errors. It is more difficult to probe the pros and cons of a data-driven technique if the input has missing values, accident dates that occur after reporting dates, large positive and negative dummy transactions that offset, or other fun and amusing diversions. Of course, reserving is chiefly a practical data exercise and at some point this means machine learning cannot demand perfect data from the actuary. However, perhaps there are interesting questions to be answered first. 8.2 Make along with me Fortunately for those interested in applying machine learning to a reserving context, Gabrielli and Wüthrich (2018) have released an infinite supply of polished datasets using a simulation approach set out in a 2018 paper. Briefly, they have built a tool in the R environment which mimics a company dataset containing twelve years of history for about ten million claims. The data is generated at a claim level and includes the paid amount each year in addition to non-financial features. For example, the data includes factors for claim code and line of business, and the machine allows some assumptions to be varied at whim. Kuo (2019) has helpfully gift-wrapped the simulation machine in an R package that allows us to easily generate simulation outputs. Lets look at some example code below. First of all, the package is not on CRAN so it must be installed from github as follows: ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:viridis&#39;: ## ## viridis_pal The paper describes the simulation approach in full mathematical detail. Here we just give a brief description of some of the less obvious fields: Field Description report_delay Difference between reporting date and occurence date in years lob Line of business (1 - 4) cc Claim code factor age ..of claimant (15 - 70) injured_part Factor coding body part injured claim_status_open Is the claim open at the end of the development year? Here is a quick look at some data snipped from the top of the table: ## claim_id accident_year development_year accident_quarter report_delay lob cc age injured_part paid_loss claim_status_open ## 1: 1 1994 0 4 0 2 6 42 36 0 1 ## 2: 1 1994 1 4 0 2 6 42 36 0 0 ## 3: 1 1994 2 4 0 2 6 42 36 0 0 ## 4: 1 1994 3 4 0 2 6 42 36 0 0 ## 5: 1 1994 4 4 0 2 6 42 36 0 0 ## 6: 1 1994 5 4 0 2 6 42 36 0 0 Pulling up a data summary is more useful and easy to do in R: ## claim_id accident_year development_year accident_quarter report_delay lob cc age injured_part ## Length:120036 Min. :1994 Min. : 0.00 Min. :1.000 Min. :0.00000 1:30636 17 :14124 Min. :15.00 36 :17412 ## Class :character 1st Qu.:1997 1st Qu.: 2.75 1st Qu.:2.000 1st Qu.:0.00000 2:29724 6 :10092 1st Qu.:24.00 51 :10824 ## Mode :character Median :2000 Median : 5.50 Median :3.000 Median :0.00000 3:29748 31 : 6936 Median :34.00 12 : 8412 ## Mean :2000 Mean : 5.50 Mean :2.507 Mean :0.09157 4:29928 19 : 6648 Mean :35.02 53 : 8328 ## 3rd Qu.:2003 3rd Qu.: 8.25 3rd Qu.:4.000 3rd Qu.:0.00000 47 : 5220 3rd Qu.:44.00 35 : 6900 ## Max. :2005 Max. :11.00 Max. :4.000 Max. :9.00000 45 : 4560 Max. :70.00 54 : 4920 ## (Other):72456 (Other):63240 ## paid_loss claim_status_open ## Min. :-43525.0 Min. :0.00000 ## 1st Qu.: 0.0 1st Qu.:0.00000 ## Median : 0.0 Median :0.00000 ## Mean : 146.1 Mean :0.09481 ## 3rd Qu.: 0.0 3rd Qu.:0.00000 ## Max. :292685.0 Max. :1.00000 ## Observations: There are 120,036 rows, close to the expected number (10,000 expected claims x 12 years of annual paid history = 120,000 rows) Note - the simulation still generates an annual paid of 0 and sets claim_status_open = 1 for the years prior to the claim being reported Claims are usually reported within a year of occurrence Age ranges from 15 to 70 and averages 35 Accident year and development year have the ranges we expect Paid is nil most of the time The claims are evenly allocated to the four lines of business, as we expect The most common injured_part and cc take up c. 10% - 15% of the data A glance at the classic paid development chart reveals a pattern that looks fairly typical: It is straightforward to go beyond the basic chart and plot the paid in finer detail - here by line of business: Next we can create a claim level summary of total paid, analyse the size distribution, and see how much average paid varies with LOB, age and reporting delay: ## claim_id lob age report_delay paid value_band ## Length:10003 1:2553 Min. :15.00 Min. :0.00000 Min. : 0.0 (-Inf,0] :2842 ## Class :character 2:2477 1st Qu.:24.00 1st Qu.:0.00000 1st Qu.: 0.0 (0,100] : 345 ## Mode :character 3:2479 Median :34.00 Median :0.00000 Median : 278.0 (100,1000] :4710 ## 4:2494 Mean :35.02 Mean :0.09157 Mean : 1752.8 (1000,10000] :1914 ## 3rd Qu.:44.00 3rd Qu.:0.00000 3rd Qu.: 787.5 (10000,100000]: 168 ## Max. :70.00 Max. :9.00000 Max. :799159.0 (100000, Inf] : 24 The paid by claim is characterised by lots of low value claims occasionally distorted by larger claims - the nil rate is around 28% and the third quartile is under half the value of the mean. Possibly we would want to consider modelling the nil claims and claims over 100k separately. ## report_delay average_paid claim_count ## 1: 0 1831 9146 ## 2: 1 920 827 ## 3: 2 510 18 ## 4: 3 519 6 ## 5: 4 5952 2 ## 6: 5 1545 2 ## 7: 8 0 1 ## 8: 9 0 1 The relationship here is unclear as around 90% of claims have a reporting delay of 0. ## lob average_paid num_claims ## 1: 1 784 2553 ## 2: 2 2050 2477 ## 3: 3 2936 2479 ## 4: 4 1273 2494 The average cost appears to vary by lob. This could be helpful information and lead to tracking the underlying lob mix, or possibly analysing the lobs separately. The average paid appears to increase up to around the age of 50. The values for ages around 55 may be worthy of further investigation, or just a small number of high value claims causing distortion. 8.3 Little by little This post has introduced an R package for accessing the Gabrielli and Wüthrich claims simulation machine and looked at one example simulation. The machine has obvious uses as a potential dartboard for comparing the accuracy of various machine learning and reserving methods, and as a playground for honing your R reserving skills. The charts and summaries presented here serve as a prompt for further analysis. Enjoy! "]]
